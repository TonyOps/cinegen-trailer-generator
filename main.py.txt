import json
import numpy as np
import random
import re
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# --- Função para converter uma cena em uma string formatada ---
def scene_to_string(scene):
    # Formata a duração com 2 casas decimais
    dur = scene['duracao_segundos']
    # Exemplo de saída: "CN1 (L - 3.41s) - NV: NV1"
    return f"{scene['elemento']} ({scene['contexto']} - {dur:.2f}s) - NV: {scene['NV']}"

# --- Carregar dados do banco (banco.json) ---
with open('banco.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Extrair sequências (um trailer = lista de cenas) e converter cada cena em string
trailers = []
for trailer in data['trailers']:
    cenas = trailer['cenas']
    scene_list = [scene_to_string(scene) for scene in cenas]
    trailers.append(scene_list)

# Para facilitar o treinamento, unimos as cenas de cada trailer em um único texto,
# usando um delimitador especial (por exemplo, "<SCENE>")
delimiter = " <SCENE> "
texts = [delimiter.join(scene_list) for scene_list in trailers]

# --- Preparação do tokenizer ---
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
total_words = len(tokenizer.word_index) + 1

# --- Criação das sequências de treinamento ---
input_sequences = []
for text in texts:
    token_list = tokenizer.texts_to_sequences([text])[0]
    # Gerar sequências n-grama para cada posição
    for i in range(1, len(token_list)):
        n_gram_seq = token_list[:i+1]
        input_sequences.append(n_gram_seq)

# Padding para que todas as sequências tenham o mesmo tamanho
max_seq_len = max(len(seq) for seq in input_sequences)
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))

# Separar em preditores (X) e o próximo token (y)
X = input_sequences[:, :-1]
y = input_sequences[:, -1]
# One-hot encoding para y
y = np.eye(total_words)[y]

# --- Construção do modelo LSTM ---
model = Sequential()
model.add(Embedding(total_words, 10, input_length=max_seq_len - 1))
model.add(LSTM(150))
model.add(Dense(total_words, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())

# Treinar o modelo – para fins de demonstração, usamos poucas épocas
model.fit(X, y, epochs=200, verbose=1)

# --- Função para gerar um trailer com o número de cenas informado ---
def generate_trailer(num_scenes):
    # Escolhe aleatoriamente um texto (de um trailer do dataset) como seed
    seed_text = random.choice(texts)
    generated_text = seed_text
    # Gerar palavras até que o número de cenas (delimitadas por "<SCENE>") seja atingido
    while generated_text.count("<SCENE>") < (num_scenes - 1):
        token_list = tokenizer.texts_to_sequences([generated_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')
        predicted_probs = model.predict(token_list, verbose=0)
        predicted_index = np.argmax(predicted_probs, axis=-1)[0]
        # Encontrar a palavra correspondente
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        generated_text += " " + output_word

    # Dividir o texto gerado em cenas e selecionar as primeiras num_scenes
    scenes_generated = [s.strip() for s in generated_text.split("<SCENE>") if s.strip() != ""]
    scenes_generated = scenes_generated[:num_scenes]

    # Gerar timestamps cumulativos e formatar cada cena conforme o exemplo
    timestamp = 0.0  # em segundos
    output_lines = []
    order = 1
    for scene_str in scenes_generated:
        # Extração do contexto e duração (espera-se o padrão: "(<Contexto> - <duracao>s)")
        match = re.search(r'\((\w)\s*-\s*([\d\.]+)s\)', scene_str)
        if match:
            contexto = match.group(1)
            dur = float(match.group(2))
        else:
            contexto = "L"
            dur = 3.0  # valor padrão se não encontrado

        # Gerar timestamp no formato HH:MM:SS:FF (supondo FF = centésimos de segundo)
        hours = int(timestamp // 3600)
        minutes = int((timestamp % 3600) // 60)
        seconds = int(timestamp % 60)
        frames = int((timestamp - int(timestamp)) * 100)
        ts_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}:{frames:02d}"

        # Formatar duração para exibição (ex.: "3.41s" e "[3s 410ms]")
        dur_str = f"{dur:.2f}s"
        sec_part = int(dur)
        ms_part = int(round((dur - sec_part) * 1000))
        detailed = f"[{sec_part}s {ms_part}ms]"

        # Tenta extrair NV do texto gerado (padrão: "NV: NVx")
        nv_match = re.search(r'NV:\s*(NV\d+)', scene_str)
        nv = nv_match.group(1) if nv_match else "NV1"

        # Compor a linha no formato final:
        # "timestamp - <scene_str> - Ordem: <order> - NV: <nv>"
        line = f"{ts_str} - {scene_str} - Ordem: {order} - NV: {nv}"
        output_lines.append(line)
        order += 1
        timestamp += dur  # atualizar timestamp

    return output_lines

# --- Menu interativo ---
if __name__ == '__main__':
    try:
        num_scenes = int(input("Digite o número de cenas desejado: "))
    except ValueError:
        print("Por favor, digite um número inteiro válido.")
        exit(1)

    generated_trailer = generate_trailer(num_scenes)
    print("\nTrailer gerado:")
    for line in generated_trailer:
        print(line)
    
    # Perguntar se o usuário deseja salvar o resultado
    save_option = input("\nDeseja salvar o resultado em um arquivo? (s/n): ").strip().lower()
    if save_option == "s":
        file_name = input("Digite o nome do arquivo (ex: trailer.txt): ").strip()
        if not file_name:
            file_name = "trailer.txt"
        with open(file_name, "w", encoding="utf-8") as f:
            for line in generated_trailer:
                f.write(line + "\n")
        print(f"Resultado salvo no arquivo '{file_name}'.")
    
    input("\nPressione Enter para sair...")
